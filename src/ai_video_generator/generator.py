import os
import requests
import asyncio
import edge_tts
from openai import OpenAI
'''å›¾åƒ & è¯­éŸ³ç”Ÿæˆå™¨æ¨¡å—'''

#æš‚æ—¶ä½¿ç”¨é€šä¹‰åƒæ–‡çš„
client = OpenAI(api_key=input("è¯·è¾“å…¥å›¾ç‰‡æ¨¡å‹API"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")


def generate_image(prompt: str, output_path: str):
    response = client.images.generate(
        model="flux-dev",
        prompt=prompt,
        size="1024x1024",
        quality="standard",
        n=1
    )
    url = response.data[0].url
    image_data = requests.get(url).content
    with open(output_path, "wb") as f:
        f.write(image_data)
    print(f"ğŸ–¼ï¸ å›¾ç‰‡å·²ç”Ÿæˆï¼š{output_path}")


async def generate_audio(text: str, output_path: str, voice="zh-CN-XiaoxiaoNeural"):
    communicate = edge_tts.Communicate(text, voice=voice)
    await communicate.save(output_path)
    print(f"ğŸ”Š è¯­éŸ³å·²ç”Ÿæˆï¼š{output_path}")


def generate_assets(sentence: str, tags: list[str], index: int, output_dir: str):
    """æ ¹æ®å¥å­å’Œtagç”Ÿæˆå›¾åƒå’ŒéŸ³é¢‘æ–‡ä»¶"""
    prompt = f"{sentence} " + " ".join([f"#{tag}" for tag in tags])
    img_path = os.path.join(output_dir, f"clip_{index:03}.jpg")
    audio_path = os.path.join(output_dir, f"clip_{index:03}.mp3")

    generate_image(prompt, img_path)
    asyncio.run(generate_audio(sentence, audio_path))

    return img_path, audio_path
